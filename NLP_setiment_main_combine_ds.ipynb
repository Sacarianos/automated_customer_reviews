{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project NLP | Business Case: Automated Customer Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Load the dataset             \n",
    "data1 = pd.read_csv('data/1429_1.csv')\n",
    "data2 = pd.read_csv('data/Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products_May19.csv')\n",
    "data3 = pd.read_csv('data/Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products.csv')\n",
    "\n",
    "# Combine the datasets\n",
    "\n",
    "data = pd.merge(data1, data2, how='outer')\n",
    "data = pd.merge(data, data3, how='outer')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Remove duplicates if necessary\n",
    "data.drop_duplicates(inplace=True)\n",
    "\n",
    "# Step 5: Save the Combined Dataset\n",
    "data.to_csv('data/combined_data.csv', index=False)\n",
    "\n",
    "print(\"Data successfully combined and saved to 'combined_data.csv'.\")\n",
    "\n",
    "\n",
    "# Inspect the first few rows of the data\n",
    "print(\"Data Head:\")\n",
    "print(data.head())\n",
    "\n",
    "# Check for any missing values\n",
    "print(\"Missing Values in Data:\")\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# Display basic statistics for the data\n",
    "print(\"Data Statistics:\")\n",
    "print(data.describe())\n",
    "\n",
    "# Display columns to understand the structure\n",
    "print(\"Data Columns:\")\n",
    "print(data.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop unecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to drop\n",
    "columns_to_drop = [\n",
    "    'id', 'name', 'asins', 'brand', 'categories', 'keys', 'manufacturer', \n",
    "    'reviews.date', 'reviews.dateAdded', 'reviews.dateSeen', 'reviews.didPurchase', \n",
    "    'reviews.id', 'reviews.sourceURLs', 'reviews.userCity', 'reviews.userProvince', 'reviews.username', 'primaryCategories',\n",
    "    'imageURLs', 'manufacturerNumber',  'sourceURLs', 'dateUpdated', 'dateAdded'\n",
    "]\n",
    "\n",
    "# Drop the columns\n",
    "data_cleaned = data.drop(columns=columns_to_drop)\n",
    "\n",
    "# Rename columns\n",
    "\n",
    "data_cleaned.columns = data_cleaned.columns.str.replace('reviews.', '', regex=False)\n",
    "\n",
    "# Display columns to confirm the drop\n",
    "print(\"Data Columns After Dropping:\")\n",
    "print(data_cleaned.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for any missing values\n",
    "print(\"Missing Values in Data:\")\n",
    "print(data_cleaned.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop rows missing text and rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing 'text' and 'rating'\n",
    "data_cleaned = data_cleaned.dropna(subset=['text', 'rating'])\n",
    "\n",
    "# Check for any remaining missing values\n",
    "print(\"Missing Values in Data After Dropping Rows:\")\n",
    "print(data_cleaned.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the Distribution of Rating and doRecommend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "# Plot the distribution of ratings\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.countplot(data=data_cleaned, x='rating')\n",
    "plt.title('Distribution of Ratings')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Plot the distribution of doRecommend\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.countplot(data=data_cleaned, x='doRecommend')\n",
    "plt.title('Distribution of Recommendations')\n",
    "plt.xlabel('Do Recommend')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Joint plot to explore correlation\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.boxplot(x='rating', y='doRecommend', data=data_cleaned)\n",
    "plt.title('Box Plot of Ratings vs. Recommendations')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Do Recommend')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Review Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the length of each review\n",
    "data_cleaned['review_length'] = data_cleaned['text'].apply(len)\n",
    "\n",
    "# Display the first few rows to confirm the new column\n",
    "print(data_cleaned[['text', 'review_length']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation between review lenght and rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Scatter plot of review length vs. rating\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.scatterplot(data=data_cleaned, x='review_length', y='rating')\n",
    "plt.title('Review Length vs. Rating')\n",
    "plt.xlabel('Review Length')\n",
    "plt.ylabel('Rating')\n",
    "plt.show()\n",
    "\n",
    "# Scatter plot of review length vs. doRecommend\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.scatterplot(data=data_cleaned, x='review_length', y='doRecommend')\n",
    "plt.title('Review Length vs. Do Recommend')\n",
    "plt.xlabel('Review Length')\n",
    "plt.ylabel('Do Recommend')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation coefficient between review length and rating\n",
    "correlation_rating = data_cleaned['review_length'].corr(data_cleaned['rating'])\n",
    "print(f'Correlation between Review Length and Rating: {correlation_rating:.2f}')\n",
    "\n",
    "# Calculate correlation coefficient between review length and doRecommend\n",
    "correlation_doRecommend = data_cleaned['review_length'].corr(data_cleaned['doRecommend'])\n",
    "print(f'Correlation between Review Length and Do Recommend: {correlation_doRecommend:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## encode ratings into positive (2), neutral (1), negative (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to categorize ratings\n",
    "def categorize_rating(rating):\n",
    "    if rating in [4, 5]:\n",
    "        return 2  # Positive\n",
    "    elif rating == 3:\n",
    "        return 1  # Neutral\n",
    "    else:\n",
    "        return 0  # Negative\n",
    "\n",
    "# Apply the function to the rating column\n",
    "data_cleaned['sentiment'] = data_cleaned['rating'].apply(categorize_rating)\n",
    "\n",
    "# Display the first few rows to confirm the new column\n",
    "print(data_cleaned[['rating', 'sentiment']].head())\n",
    "\n",
    "# Check the distribution of the new categories\n",
    "print(\"Distribution of Rating Categories:\")\n",
    "print(data_cleaned['sentiment'].value_counts())\n",
    "\n",
    "# Plot the distribution of Sentiment\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.countplot(data=data_cleaned, x='sentiment')\n",
    "plt.title('Distribution of Sentiment')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# Generate word clouds\n",
    "negative = ' '.join(data_cleaned[data_cleaned['sentiment'] == 0]['text'])\n",
    "neutral = ' '.join(data_cleaned[data_cleaned['sentiment'] == 1]['text'])\n",
    "positive = ' '.join(data_cleaned[data_cleaned['sentiment'] == 2]['text'])\n",
    "\n",
    "# Word cloud for negatige reviews\n",
    "wordcloud_negative = WordCloud(width=800, height=400, background_color='white').generate(negative)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud_negative, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Word Cloud for Negative Reviews')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Word cloud for neutral reviews\n",
    "wordcloud_neutral = WordCloud(width=800, height=400, background_color='white').generate(neutral)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud_neutral, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Word Cloud for Neutral Reviews')\n",
    "plt.show()\n",
    "\n",
    "# Word cloud for positive reviews\n",
    "wordcloud_positive = WordCloud(width=800, height=400, background_color='white').generate(positive)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud_positive, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Word Cloud for Real News')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning (Lowercase, Remove Stopwords, Lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "# Download NLTK stopwords and WordNet data if not already done\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Define additional stop words \n",
    "additional_stop_words = {'tablet', 'amazon', 'kindle', 'bought', 'one', 'use'}\n",
    "\n",
    "\n",
    "# Define the preprocessing function\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove special characters\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra whitespace\n",
    "    stop_words = set(stopwords.words('english')).union(additional_stop_words)\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])  # Remove stopwords\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])  # Lemmatization\n",
    "    return text\n",
    "\n",
    "# Apply preprocessing\n",
    "data_cleaned['cleaned_text'] = data_cleaned['text'].apply(preprocess_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Feature matrix (X) and target vector (Y)\n",
    "X = data_cleaned['cleaned_text']\n",
    "Y = data_cleaned['sentiment']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Vectorize the text data\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SMOTE to balance the training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_vectorized, y_train)\n",
    "\n",
    "# Confirm the resampling\n",
    "print(\"Original training set class distribution:\")\n",
    "print(y_train.value_counts())\n",
    "print(\"\\nResampled training set class distribution:\")\n",
    "print(pd.Series(y_train_resampled).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "import pandas as pd\n",
    "\n",
    "# Function to evaluate model performance\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    print(f'Accuracy: {accuracy:.2f}')\n",
    "    print(f'Precision: {precision:.2f}')\n",
    "    print(f'Recall: {recall:.2f}')\n",
    "    print(f'F1 Score: {f1:.2f}')\n",
    "    print('Confusion Matrix:')\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print('Classification Report:')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Train and evaluate models\n",
    "def train_and_evaluate_models(X_train_resampled, y_train_resampled, X_test_vectorized, y_test):\n",
    "    models = {\n",
    "        'Random Forest': RandomForestClassifier(random_state=42),\n",
    "        'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
    "        'XGBoost': XGBClassifier(random_state=42),\n",
    "        'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42)\n",
    "    }\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f'\\nTraining {name}...')\n",
    "        model.fit(X_train_resampled, y_train_resampled)\n",
    "        print(f'Evaluating {name}...')\n",
    "        evaluate_model(model, X_test_vectorized, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate models\n",
    "train_and_evaluate_models(X_train_resampled, y_train_resampled, X_test_vectorized, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Weight Adjustment in Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Define the model with class weights\n",
    "class_weights = {0: 2, 1: 2, 2: 1}\n",
    "rf_model = RandomForestClassifier(random_state=42, class_weight=class_weights)\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(rf_model, X_test_vectorized, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biderectional LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "\n",
    "# Encode target labels\n",
    "label_encoder = LabelEncoder()\n",
    "Y_encoded = label_encoder.fit_transform(Y)\n",
    "Y_categorical = to_categorical(Y_encoded)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y_categorical, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenize and pad sequences\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "max_sequence_length = max([len(x) for x in X_train_sequences])\n",
    "X_train_padded = pad_sequences(X_train_sequences, maxlen=max_sequence_length)\n",
    "X_test_padded = pad_sequences(X_test_sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# Build the Bidirectional LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=5000, output_dim=128, input_length=max_sequence_length))\n",
    "model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_padded, y_train, epochs=10, batch_size=64, validation_data=(X_test_padded, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test_padded, y_test)\n",
    "print(f'Test Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_padded)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_test_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Print classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test_classes, y_pred_classes, target_names=label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning with Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Initialize the Random Forest model with class weights\n",
    "rf = RandomForestClassifier(class_weight='balanced', random_state=42)\n",
    "\n",
    "# Initialize the Grid Search model\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2, scoring='f1_weighted')\n",
    "\n",
    "# Fit the Grid Search model\n",
    "grid_search.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Get the best parameters and the best score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(f'Best Parameters: {best_params}')\n",
    "print(f'Best F1 Score: {best_score}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
